{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQalREmJx0nJy1MN771FMo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6mMaWtZRP7LQ","executionInfo":{"status":"ok","timestamp":1690998743135,"user_tz":240,"elapsed":55223,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"53fad236-2db5-487c-a400-5bbe5389ffd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Standard libraries\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from tensorflow.keras import layers, models\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","import librosa\n","from google.colab import drive\n","import os\n","\n","from librosa import load as lload\n","from librosa.feature import mfcc\n","from librosa.feature.inverse import mfcc_to_audio\n","\n","# Mount Google Drive for access\n","drive.mount('/content/drive')\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# Base directory path\n","base_directory_path = '/content/drive/MyDrive/207-Project/notebooks/RG/3_species/librosa_loaded_sr16000/'\n","\n"]},{"cell_type":"code","source":["\n","# Function to extract 5-second  chunks from audio\n","def extract_5sec_chunks(\n","    audio_array: np.ndarray,\n","    window_size_s: float = 5.0,\n","    hop_size_s: float = 2.5, #(with overlap)\n","    sample_rate=16000,\n","    # n_fft=2048,\n","    # n_mfcc=13,\n",") -> np.ndarray:\n","    frame_length = int(window_size_s * sample_rate)\n","    frame_step = int(hop_size_s * sample_rate)\n","    framed_audio = tf.signal.frame(audio_array, frame_length, frame_step, pad_end=False)\n","\n","    return framed_audio\n","\n"],"metadata":{"id":"ye2zxAsNVbHD","executionInfo":{"status":"ok","timestamp":1690998747693,"user_tz":240,"elapsed":125,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Load metadata\n","dataset_path = '/content/drive/MyDrive/207-Project/notebooks/RG/3_species/'\n","metadata_path = os.path.join(dataset_path, \"train_val.csv\")\n","metadata_df = pd.read_csv(metadata_path)\n","metadata_df = metadata_df[metadata_df['duration_secs_32000'] >= 8]\n","\n","# Create a dictionary to map filenames to labels\n","labels_dict = metadata_df.set_index('filename_npy')['primary_label'].to_dict()\n","\n","# Get unique filenames from the metadata\n","filenames = set(base_directory_path + '/' + metadata_df['filename_npy'].unique())\n","\n","# Split train and validation data\n","train_df = metadata_df[metadata_df['data'] == 'train']\n","validate_df = metadata_df[metadata_df['data'] == 'val']"],"metadata":{"id":"8tFLnsfwVfsf","executionInfo":{"status":"ok","timestamp":1690998756092,"user_tz":240,"elapsed":1903,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def extract_final_features_from_dataframe(df):\n","    mfccs = []\n","    target_labels = []\n","    for index, row in df.iterrows():\n","        class_label = row[\"primary_label\"]\n","\n","        # Extract MFCC and spectral centroid features for each frame\n","        mfcc_frames, target_label = extract_mfcc_and_spectral_centroid_from_frames(framed_audio, class_label, sample_rate=16000)\n","\n","        mfccs.extend(mfcc_frames)\n","        target_labels.extend(target_label)\n","    return mfccs, target_labels"],"metadata":{"id":"cKnpolCgDBw7","executionInfo":{"status":"ok","timestamp":1690998759550,"user_tz":240,"elapsed":162,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#scale data\n","def scale_data(data):\n","    # Reshape the data to 2D array\n","    num_samples, num_time_steps, num_features = data.shape\n","    data_reshaped = data.reshape(num_samples, num_features * num_time_steps)\n","\n","    # Create the MinMaxScaler object and fit it to the data\n","    scaler = MinMaxScaler(feature_range=(0, 1))\n","    data_scaled = scaler.fit_transform(data_reshaped)\n","\n","    # Reshape the scaled data back to its original shape\n","    data_scaled = data_scaled.reshape(num_samples, num_time_steps, num_features)\n","\n","    return data_scaled"],"metadata":{"id":"GjneO_xLXtXR","executionInfo":{"status":"ok","timestamp":1690998762140,"user_tz":240,"elapsed":111,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# def mfcc_chunks(dataframe: pd.DataFrame, sample_rate=16000, n_mfcc=13, n_fft=2048) -> tuple[np.ndarray, np.ndarray]:\n","    # y = []\n","    # X = []\n","\n","    # for i, row in dataframe.iterrows():\n","    #   label = row['primary_label']\n","    #   tensor_5sec = row['audio_chunks']\n","\n","    #   for each in tensor_5sec:\n","    #     y.append(label)\n","    #     each = np.array(each)\n","    #     X_mfcc = mfcc(y=each, sr=sample_rate)\n","    #     X.append(X_mfcc)\n","\n","    # assert len(y) == len(X)\n","\n","    # y = np.array(y)\n","    # X = np.array(X)\n","\n","\n","\n","    # return X, y\n","\n","def extract_mfcc_and_spectral_centroid_from_frames(framed_audio, class_label, sample_rate=16000, n_mfcc=20, n_chroma=12):\n","    mfcc_frames = []\n","    spectral_centroid_frames = []\n","    target_label = []\n","\n","    for frame in framed_audio:\n","        frame = np.array(frame)\n","\n","        # Extract MFCC from the main audio frame\n","        mfcc = librosa.feature.mfcc(y=frame, sr=sample_rate, n_mfcc=n_mfcc)\n","        transposed_mfcc = mfcc.T\n","\n","        # Extract spectral centroid from the main audio frame\n","        spectral_centroid = librosa.feature.spectral_centroid(y=frame, sr=sample_rate)\n","        transposed_spectral_centroid = spectral_centroid.T\n","\n","        # Append the features to the respective lists\n","        mfcc_frames.append(transposed_mfcc)\n","        spectral_centroid_frames.append(transposed_spectral_centroid)\n","        target_label.append(class_label)\n","\n","    # Combine MFCC and spectral centroid features along the time axis\n","    combined_frames = [np.hstack((mfcc, spectral_centroid)) for mfcc, spectral_centroid in zip(mfcc_frames, spectral_centroid_frames)]\n","    combined_frames = np.array(combined_frames)\n","\n","    # Convert the lists to numpy arrays\n","    target_label = np.array(target_label)\n","\n","    return combined_frames, target_label"],"metadata":{"id":"71lQ2yKEpajb","executionInfo":{"status":"ok","timestamp":1690998766419,"user_tz":240,"elapsed":111,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Load all train audio data one time\n","train_audios = []\n","for filename in train_df['filename_npy']:\n","    audio = np.load('/content/drive/MyDrive/207-Project/data/train/librosa_loaded/' + filename)\n","    train_audios.append(audio)"],"metadata":{"id":"8RqTMrMFdmmQ","executionInfo":{"status":"ok","timestamp":1690998985754,"user_tz":240,"elapsed":216477,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Load all validate audio data one time\n","val_audios = []\n","for filename in validate_df['filename_npy']:\n","    audio = np.load('/content/drive/MyDrive/207-Project/data/train/librosa_loaded/' + filename)\n","    val_audios.append(audio)"],"metadata":{"id":"z5qgBRCzF1QM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_audios))\n","print(len(val_audios))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDbggtfjd2Hr","executionInfo":{"status":"ok","timestamp":1690429898588,"user_tz":240,"elapsed":54,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"b725a901-0a9b-4824-aa3b-271b926e5188"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["657\n","283\n"]}]},{"cell_type":"code","source":["#Initialize an empty list to store the chunk data for training data\n","train_chunks = []\n","\n","for audio in train_audios:\n","  train_chunks.append(extract_5sec_chunks(audio))\n","\n","len(train_chunks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ix5SF5KId60e","executionInfo":{"status":"ok","timestamp":1690429903267,"user_tz":240,"elapsed":4688,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"a53f6492-357b-4dac-8800-d78ddf0d2b65"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["657"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#Initialize an empty list to store the chunk data for VALIDATION data\n","val_chunks = []\n","\n","for audio in val_audios:\n","  val_chunks.append(extract_5sec_chunks(audio))\n","\n","len(val_chunks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLEFOI9PGE1E","executionInfo":{"status":"ok","timestamp":1690429905037,"user_tz":240,"elapsed":1800,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"5a1ebae4-8985-4ec0-a539-be451fb88758"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["283"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Extract the mfcc embeddings from the model\n","train_embeddings, labels = extract_final_features_from_dataframe(train_chunks)"],"metadata":{"id":"MIrf6wceX9Gu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#add chunks to trainig data\n","train_df['audio_chunks'] = train_chunks\n","train_df.head()\n","train_df = train_df.sample(frac=1, random_state=1234)\n","\n","\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1690429905037,"user_tz":240,"elapsed":32,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"oR4bZREIeH11","outputId":"2fc44bae-9481-4c17-8a34-76c7d8317877"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-10-3809d4e98fc2>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  train_df['audio_chunks'] = train_chunks\n"]}]},{"cell_type":"code","source":["#add chunks to validation data\n","validate_df['audio_chunks'] = val_chunks\n","validate_df.head()\n","validate_df = validate_df.sample(frac=1, random_state=1234)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ay_bQ3nTGjRj","executionInfo":{"status":"ok","timestamp":1690429905038,"user_tz":240,"elapsed":22,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"69c5efba-9f31-42c8-90da-c39a80d63af7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-11-0cb3c619c1e6>:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  validate_df['audio_chunks'] = val_chunks\n"]}]},{"cell_type":"code","source":["%who\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrqnOGEc4Hhx","executionInfo":{"status":"ok","timestamp":1690429905039,"user_tz":240,"elapsed":15,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"a8420952-3596-4c2a-c160-308bf3e3ccab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LabelEncoder\t MinMaxScaler\t StandardScaler\t audio\t base_directory_path\t dataset_path\t drive\t extract_5sec_chunks\t filename\t \n","filenames\t labels_dict\t layers\t librosa\t lload\t metadata_df\t metadata_path\t mfcc\t mfcc_chunks\t \n","mfcc_to_audio\t models\t np\t os\t pd\t tf\t train_audios\t train_chunks\t train_df\t \n","train_test_split\t val_audios\t val_chunks\t validate_df\t \n"]}]},{"cell_type":"code","source":["del audio\n","del base_directory_path\n","del dataset_path\n","del drive\n","del extract_5sec_chunks\n","del filename\n","del filenames\n","# del labels_dict\n","del layers\n","del metadata_df\n","del metadata_path\n","del os\n","del train_audios\n","del train_chunks\n","del val_audios\n","del val_chunks\n","del train_test_split\n"],"metadata":{"id":"XqB3TMMN4PWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","gc.collect()"],"metadata":{"id":"c2kC8F9w42J1","executionInfo":{"status":"ok","timestamp":1690429906040,"user_tz":240,"elapsed":30,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"ac36a640-dec1-4d3e-a383-1978dcc95c18","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["28549"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["%who\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ga1OuGvb5w95","executionInfo":{"status":"ok","timestamp":1690429906041,"user_tz":240,"elapsed":15,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"337a33ac-0b3a-4ba7-8ce1-b3d244881d72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LabelEncoder\t MinMaxScaler\t StandardScaler\t gc\t labels_dict\t librosa\t lload\t mfcc\t mfcc_chunks\t \n","mfcc_to_audio\t models\t np\t pd\t tf\t train_df\t validate_df\t \n"]}]},{"cell_type":"code","source":["#convert to X and y\n","X_train, y_train = mfcc_chunks(train_df)"],"metadata":{"id":"kB9deDbllmIl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert to X and y\n","X_val, y_val = mfcc_chunks(validate_df)"],"metadata":{"id":"Qx0m86C2GugA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"X_val shape:\", X_val.shape)\n","print(\"y_val shape:\", y_val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DxmyoggZP8eu","executionInfo":{"status":"ok","timestamp":1690430080416,"user_tz":240,"elapsed":20,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"fc5b5a29-41f2-42fe-dbfd-cb505b3b8a7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (7889, 20, 157)\n","y_train shape: (7889,)\n","X_val shape: (3121, 20, 157)\n","y_val shape: (3121,)\n"]}]},{"cell_type":"code","source":["# validate_df.head()\n","print(train_df['primary_label'].unique())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGxFxhm_Nfz-","executionInfo":{"status":"ok","timestamp":1690430080416,"user_tz":240,"elapsed":9,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"4bc2aef5-9028-49c1-8508-a65bd9a8b207"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comsan' 'eaywag1' 'barswa']\n"]}]},{"cell_type":"code","source":["del train_df\n","del validate_df\n","del mfcc\n","del mfcc_chunks\n","del mfcc_to_audio"],"metadata":{"id":"caNxSmyVbKUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json"],"metadata":{"id":"OlH7rWgpQLW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_list = X_train.tolist()\n","y_train_list = y_train.tolist()\n","\n","X_val_list = X_val.tolist()\n","y_val_list = y_val.tolist()\n","\n","# Save data to JSON files\n","with open('/content/drive/MyDrive/My-207/X_train_mfcc_aug.json', 'w') as file:\n","    json.dump(X_train_list, file)\n","\n","with open('/content/drive/MyDrive/My-207/y_train_mfcc_aug.json', 'w') as file:\n","    json.dump(y_train_list, file)\n","\n","with open('/content/drive/MyDrive/My-207/X_val_mfcc_aug.json', 'w') as file:\n","    json.dump(X_val_list, file)\n","\n","with open('/content/drive/MyDrive/My-207/y_val_mfcc_aug.json', 'w') as file:\n","    json.dump(y_val_list, file)"],"metadata":{"id":"xXaMLhMmQNyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data normalization\n","# Create the scaler\n","scaler = MinMaxScaler()\n","\n","# Flatten X_train into a 2-dimensional array\n","X_train = X_train.reshape(X_train.shape[0], -1)\n","# Fit and transform the training data\n","X_train = scaler.fit_transform(X_train)\n","\n","# Flatten X_val into a 2-dimensional array\n","X_val = X_val.reshape(X_val.shape[0], -1)\n","\n","# Transform the validation data using the scaler parameters from the training data\n","X_val = scaler.transform(X_val)\n","\n","# Convert string labels to numerical labels for training and validation data\n","label_encoder = LabelEncoder()\n","all_labels = np.array(list(labels_dict.values()))\n","label_encoder.fit(all_labels)\n","y_train_encoded = label_encoder.transform(y_train)\n","y_val_encoded = label_encoder.transform(y_val)\n"],"metadata":{"id":"hAXEPZ_2Knt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_list = X_train.tolist()\n","y_train_list = y_train.tolist()\n","\n","X_val_list = X_val.tolist()\n","y_val_list = y_val.tolist()\n","\n","# Save data to JSON files\n","with open('/content/drive/MyDrive/My-207/X_train_n.json', 'w') as file:\n","    json.dump(X_train_list, file)\n","\n","with open('/content/drive/MyDrive/My-207/y_train_n.json', 'w') as file:\n","    json.dump(y_train_list, file)\n","\n","with open('/content/drive/MyDrive/My-207/X_val_n.json', 'w') as file:\n","    json.dump(X_val_list, file)\n","\n","with open('/content/drive/MyDrive/My-207/y_val_n.json', 'w') as file:\n","    json.dump(y_val_list, file)"],"metadata":{"id":"1xxHQBUPkzu8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Data normalization\n","# from sklearn.preprocessing import StandardScaler\n","# scaler = StandardScaler()\n","# X_train_normalized = scaler.fit_transform(X_train)\n","# X_val_normalized = scaler.transform(X_val)\n","\n","# # Convert string labels to numerical labels for training and validation data\n","# label_encoder = LabelEncoder()\n","# all_labels = np.array(list(labels_dict.values()))\n","# label_encoder.fit(all_labels)\n","# y_train_encoded = label_encoder.transform(y_train)\n","# # y_val_encoded = label_encoder.transform(y_val)\n","\n","# # # Train the model\n","# # history = model.fit(X_train_normalized, y_train_encoded, validation_data=(X_val_normalized, y_val_encoded), batch_size=32, epochs=10)\n","\n","# # Data normalization\n","# from sklearn.preprocessing import StandardScaler\n","# # Reshape X_train to 2D array\n","# num_samples, num_chunks, num_mfcc_features = X_train.shape\n","# X_train_reshaped = X_train.reshape(-1, num_mfcc_features)\n","\n","# # Now apply the StandardScaler\n","# scaler = StandardScaler()\n","# X_train = scaler.fit_transform(X_train_reshaped)\n","\n","# # Reshape X_val to 2D array\n","# num_samples, num_chunks, num_mfcc_features = X_val.shape\n","# X_val_reshaped = X_val.reshape(-1, num_mfcc_features)\n","\n","# # Now apply the StandardScaler using the same scaler fitted on X_train\n","# X_val = scaler.transform(X_val_reshaped)\n","\n","# # Convert string labels to numerical labels for training and validation data\n","# label_encoder = LabelEncoder()\n","# all_labels = np.array(list(labels_dict.values()))\n","# label_encoder.fit(all_labels)\n","# y_train = label_encoder.transform(y_train)\n","# y_val = label_encoder.transform(y_val)"],"metadata":{"id":"VOeFjmKGWeZy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(labels_dict)"],"metadata":{"id":"eTobazxBc1t9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# # Build network topology\n","# model = tf.keras.Sequential([\n","#     # Input layer\n","#     tf.keras.layers.Flatten(input_shape=X_train.shape[1:]),\n","\n","#     # 1st dense layer\n","#     tf.keras.layers.Dense(512, activation='relu'),\n","\n","#     # 2nd dense layer\n","#     tf.keras.layers.Dense(256, activation='relu'),\n","\n","#     # 3rd dense layer\n","#     tf.keras.layers.Dense(64, activation='relu'),\n","\n","#     # Output layer\n","#     tf.keras.layers.Dense(3, activation='softmax')\n","# ])\n"],"metadata":{"id":"2V8lU0YjL0yx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# # Compile model\n","# optimiser = tf.keras.optimizers.Adam(learning_rate=0.0001)\n","# model.compile(optimizer=optimiser, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# # Summary of the model\n","# model.summary()\n"],"metadata":{"id":"Ka_imzY6OF-I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# # # Train the model\n","# history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10)"],"metadata":{"id":"oa2y6OOjOH0l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"X_val shape:\", X_val.shape)\n","print(\"y_val shape:\", y_val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4liRojAkOLMx","executionInfo":{"status":"ok","timestamp":1690430244326,"user_tz":240,"elapsed":11,"user":{"displayName":"Amina Alavi","userId":"02052827403344407239"}},"outputId":"18408516-6a23-474e-ed99-3a1e4dd3aa29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (7889, 3140)\n","y_train shape: (7889,)\n","X_val shape: (3121, 3140)\n","y_val shape: (3121,)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"VkYi2sCqORec"},"execution_count":null,"outputs":[]}]}